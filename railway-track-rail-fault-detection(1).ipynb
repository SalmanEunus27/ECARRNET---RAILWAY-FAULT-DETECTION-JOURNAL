{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --upgrade pip\n!pip install tensorflow\nimport os","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","id":"I-65SOeQZ79d","outputId":"7c7e5e8d-ea7c-4fda-bd3c-b574b4b20444","execution":{"iopub.status.busy":"2021-11-09T11:08:12.872002Z","iopub.execute_input":"2021-11-09T11:08:12.872575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Defining each of these directories","metadata":{"id":"GLfwYtthZ79g"}},{"cell_type":"code","source":"train_dir = os.path.join('../input/railway-track-fault-detection-dataset1-rail/Dataset 1_Rail/Train')\nvalidation_dir = os.path.join('../input/railway-track-fault-detection-dataset1-rail/Dataset 1_Rail/Validiation')\ntest_dir = os.path.join('../input/railway-track-fault-detection-dataset1-rail/Dataset 1_Rail/Test')\n\n# Directory with our training defective/nondefective pictures\ntrain_defective_dir = os.path.join('../input/railway-track-fault-detection-dataset1-rail/Dataset 1_Rail/Train/Defective')\ntrain_nondefective_dir = os.path.join('../input/railway-track-fault-detection-dataset1-rail/Dataset 1_Rail/Train/Non Defective')\n# Directory with our validation defective/nondefective pictures\nvalidation_defective_dir = os.path.join('../input/railway-track-fault-detection-dataset1-rail/Dataset 1_Rail/Validiation/Defective')\nvalidation_nondefective_dir = os.path.join('../input/railway-track-fault-detection-dataset1-rail/Dataset 1_Rail/Validiation/Non Defective')\n\ntest_defective_dir = os.path.join('../input/railway-track-fault-detection-dataset1-rail/Dataset 1_Rail/Test/Defective')\ntest_nondefective_dir = os.path.join('../input/railway-track-fault-detection-dataset1-rail/Dataset 1_Rail/Test/Non Defective')","metadata":{"id":"TA5-pn1FZ79g","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Now, let's see what the filenames look like in the training directories:**","metadata":{"id":"7lMYvDY6Z79h"}},{"cell_type":"code","source":"train_defective_fnames = os.listdir(train_defective_dir )\ntrain_nondefective_fnames = os.listdir( train_nondefective_dir)\n\nprint(train_defective_fnames[:20])\nprint(train_nondefective_fnames[:20])","metadata":{"id":"_TtVbKrWZ79h","outputId":"544c8edb-1375-456e-ffeb-c34e73c7472e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('total training defective images :', len(os.listdir(train_defective_dir)))\nprint('total training non-defective images :', len(os.listdir(train_nondefective_dir)))\n\n\nprint('total validation defective images :', len(os.listdir( validation_defective_dir ) ))\nprint('total validation non-defective images :', len(os.listdir( validation_nondefective_dir) ))\n\nprint('total test defective images :', len(os.listdir( validation_defective_dir ) ))\nprint('total test non-defective images :', len(os.listdir( validation_nondefective_dir) ))","metadata":{"id":"JfDslQoTZ79h","outputId":"d7d9d99b-605b-413a-9000-c1e18a54bc7f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"dimensionality**Now let's take a look at a few pictures to get a better sense of what they look like. First, configure the matplot parameters:**","metadata":{"id":"fNhhLB0HZ79i"}},{"cell_type":"code","source":"%matplotlib inline\n\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\n# Parameters for our graph; we'll output images in a 10x10 configuration\nnrows = 10\nncols = 10\n\n# Index for iterating over images\npic_index = 0","metadata":{"id":"aESt0vkQZ79i","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set up matplotlib fig, and size it to fit 4x4 pics\nfig = plt.gcf()\nfig.set_size_inches(ncols * 5, nrows * 5)\n\npic_index += 10\nnext_defective_pix = [os.path.join(train_defective_dir, fname) \n                for fname in train_defective_fnames[pic_index-10:pic_index]]\nnext_nondefective_pix = [os.path.join(train_nondefective_dir, fname) \n                for fname in train_nondefective_fnames[pic_index-10:pic_index]]\n\n\nfor i, img_path in enumerate(next_defective_pix+next_nondefective_pix):\n  # Set up subplot; subplot indices start at 1\n  sp = plt.subplot(nrows, ncols, i + 1)\n  sp.axis('Off') # Don't show axes (or gridlines)\n\n  img = mpimg.imread(img_path)\n  plt.imshow(img)\n\nplt.show()","metadata":{"id":"uCBz-qnxZ79i","outputId":"42245658-97b5-4705-dc66-673ddc7025d8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf","metadata":{"id":"N2IYvryoZ79j","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The \"output shape\" column shows how the size of your feature map evolves in each successive layer. The convolution layers reduce the size of the feature maps by a bit due to padding, and each pooling layer halves the dimensions.**","metadata":{"id":"Ngil93keZ79j"}},{"cell_type":"markdown","source":"#  Building the Model","metadata":{"id":"qt8ShRrkZ79j"}},{"cell_type":"code","source":"#MUL 1 - Inception - ST\n#import keras\n# from keras.applications import InceptionV3\n# from keras.applications import Xception\nfrom tensorflow.keras.applications import InceptionResNetV2\n#from keras.applications import V2\n#model = VGG16()\nfrom  tensorflow.keras.models import Model\n# from keras.layers import concatenate\nfrom  tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Embedding, LSTM, GRU, Reshape, Concatenate,Bidirectional, Conv2D, MaxPooling2D, UpSampling2D, Flatten\n#from  tensorflow.keras.applications.mobilenet import preprocess_input\n\nfrom  tensorflow.keras.layers import GaussianNoise\ninput_img = Input(shape = (600, 600, 3))\n#f1_x = tensorflow.keras.layers.Flatten()\n#encoder\nf1_x = Conv2D(128, (6, 6), activation='relu', kernel_initializer='he_normal', padding='valid')(input_img) #28 x 28 x 32\ntf.keras.layers.BatchNormalization()\ntf.keras.layers.Dropout(rate=0.4)\nf1_x = MaxPooling2D(pool_size=(2, 2))(f1_x) #14 x 14 x 32\ntf.keras.layers.BatchNormalization()\ntf.keras.layers.Dropout(rate=0.35)\nf1_x = Conv2D(64, (4, 4), activation='relu', kernel_initializer='he_normal', padding='valid')(f1_x) #14 x 14 x 64\ntf.keras.layers.BatchNormalization()\ntf.keras.layers.Dropout(rate=0.33)\nf1_x = MaxPooling2D(pool_size=(2, 2))(f1_x) #7 x 7 x 64\ntf.keras.layers.BatchNormalization()\ntf.keras.layers.Dropout(rate=0.3)\nf1_x = Conv2D(64, (4, 4), activation='relu', padding='valid',kernel_initializer='he_normal')(f1_x) #7 x 7 x 128 (small and thick)\ntf.keras.layers.BatchNormalization()\ntf.keras.layers.Dropout(rate=0.4)\nf1_x = MaxPooling2D(pool_size=(2, 2))(f1_x)\ntf.keras.layers.BatchNormalization()\ntf.keras.layers.Dropout(rate=0.49)\n#decoder\nf1_x = Conv2D(64, (6, 6), activation='relu', padding='valid',kernel_initializer='he_normal')(f1_x) #7 x 7 x 128\ntf.keras.layers.BatchNormalization()\ntf.keras.layers.Dropout(rate=0.36)\nf1_x = UpSampling2D((2,2))(f1_x) # 14 x 14 x 128\ntf.keras.layers.BatchNormalization()\ntf.keras.layers.Dropout(rate=0.38)\nf1_x = Conv2D(128, (6, 6), activation='relu', padding='valid',kernel_initializer='he_normal')(f1_x) # 14 x 14 x 64\ntf.keras.layers.BatchNormalization()\n#dropout=0.2\nf1_x = UpSampling2D((2,2))(f1_x) # 28 x 28 x 64\ntf.keras.layers.BatchNormalization()\ntf.keras.layers.Dropout(rate=0.37)\nf1_x = Conv2D(1, (6, 6), activation='sigmoid', padding='valid', kernel_initializer='he_normal')(f1_x) # 28 x 28 x 1\ntf.keras.layers.BatchNormalization()\ntf.keras.layers.BatchNormalization()\n\ntf.keras.layers.Dropout(rate=0.5)\n#f1_x = MinPooling2D(pool_size=(3, 3))(f1_x)     \n# f1_base = Xception(weights='imagenet', include_top=False, input_shape=(450,450,3))\n# f1_base = InceptionV3(weights='imagenet', include_top=False, input_shape=(299,299,3))\nf1_base = InceptionResNetV2(weights='imagenet', include_top=False, input_shape=(600,600,3))\ntf.keras.layers.Dropout(rate=0.45)\ntf.keras.layers.BatchNormalization()\ntf.keras.layers.Dropout(rate=0.5)\ntf.keras.layers.BatchNormalization()\nf1_x = f1_base.output\ntf.keras.layers.BatchNormalization()\nf1_x = GlobalAveragePooling2D()(f1_x)\ntf.keras.layers.BatchNormalization()\ntf.keras.layers.Dropout(rate=0.4)\n#f1_x = Conv2D(128, (3, 3), activation='relu', padding='same')(f1_x) #7 x 7 x 128\n#f1_x = MaxPooling2D(pool_size=(2, 2))(f1_x) \nf1_x = Reshape([1,1536])(f1_x)  \nf1_x = Bidirectional(tf.keras.layers.LSTM(2900, \nreturn_sequences=False, \ndropout=0.55),\ninput_shape=[1,1024],\nmerge_mode='concat')(f1_x)\ntf.keras.layers.BatchNormalization()\n#f1_x = Conv2D(128, (3, 3), activation='relu', padding='same')(f1_x) #7 x 7 x 128\n#f1_x = MaxPooling2D(pool_size=(2, 2))(f1_x) \n#Regularization with noise\nf1_x = GaussianNoise(0.20)(f1_x)\ntf.keras.layers.BatchNormalization()\nf1_x = Dense(100,activation='relu', kernel_initializer='he_normal',kernel_regularizer=tf.keras.regularizers.l1_l2(0.01),kernel_constraint=tf.keras.constraints.max_norm(1.))(f1_x)\ntf.keras.layers.BatchNormalization()\ntf.keras.layers.Dropout(rate=0.5)\ntf.keras.layers.BatchNormalization()\nf1_x = Dense(100,activation='relu', kernel_initializer='he_normal',kernel_constraint=tf.keras.constraints.max_norm(1.))(f1_x)\ntf.keras.layers.Dropout(rate=0.58)\ntf.keras.layers.BatchNormalization()\ntf.keras.layers.BatchNormalization()\ntf.keras.layers.BatchNormalization()\ntf.keras.layers.Dropout(rate=0.46)\ntf.keras.layers.BatchNormalization()\n#he_avg_int = keras.initializers.VarianceScaling(scale=2, mode = 'fan_avg', distribution='uniform')\n#f1_x = Conv2D(64, (3, 3), activation='relu', padding='same')(f1_x)\nf1_x = Dense(1, activation='sigmoid', kernel_initializer='he_normal')(f1_x)\nmodel_1 = Model(inputs=[f1_base.input],outputs=[f1_x])\nprint(model_1.summary())","metadata":{"id":"Un0AmXeWEuZ4","outputId":"9f92fadb-9b04-4d0c-d5b9-ba6a410a7a7a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## fix for multi_gpu_model prediction time longer\nfrom tensorflow.keras.layers import Lambda, concatenate\nfrom tensorflow.keras import Model\nimport tensorflow as tf\n\ndef multi_gpu_model(model, gpus):\n    if isinstance(gpus, (list, tuple)):\n        num_gpus = len(gpus)\n        target_gpu_ids = gpus\n    else:\n        num_gpus = gpus\n        target_gpu_ids = range(num_gpus)\n\n    def get_slice(data, i, parts):\n        shape = tf.shape(data)\n        batch_size = shape[:1]\n        input_shape = shape[1:]\n        step = batch_size // parts\n        if i == num_gpus - 1:\n            size = batch_size - step * i\n        else:\n            size = step\n        size = tf.concat([size, input_shape], axis=0)\n        stride = tf.concat([step, input_shape * 0], axis=0)\n        start = stride * i\n        return tf.slice(data, start, size)\n\n    all_outputs = []\n    for i in range(len(model.outputs)):\n        all_outputs.append([])\n\n    # Place a copy of the model on each GPU,\n    # each getting a slice of the inputs.\n    for i, gpu_id in enumerate(target_gpu_ids):\n        with tf.device('/gpu:%d' % gpu_id):\n            with tf.name_scope('replica_%d' % gpu_id):\n                inputs = []\n                # Retrieve a slice of the input.\n                for x in model.inputs:\n                    input_shape = tuple(x.get_shape().as_list())[1:]\n                    slice_i = Lambda(get_slice,\n                                   output_shape=input_shape,\n                                   arguments={'i': i,\n                                              'parts': num_gpus})(x)\n                    inputs.append(slice_i)\n\n                # Apply model on slice\n                # (creating a model replica on the target device).\n                outputs = model(inputs)\n                if not isinstance(outputs, list):\n                    outputs = [outputs]\n\n                # Save the outputs for merging back together later.\n                for o in range(len(outputs)):\n                    all_outputs[o].append(outputs[o])\n\n    # Merge outputs on CPU.\n    with tf.device('/cpu:0'):\n        merged = []\n        for name, outputs in zip(model.output_names, all_outputs):\n            merged.append(concatenate(outputs,\n                                    axis=0, name=name))\n        return Model(model.inputs, merged)","metadata":{"id":"9TP6Q7a5Z79k","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import Callback\nimport pickle\nimport sys\n\n#Stop training on val_acc\nclass EarlyStoppingByAccVal(Callback):\n    def __init__(self, monitor='val_acc', value=0.00001, verbose=1):\n        super(Callback, self).__init__()\n        self.monitor = monitor\n        self.value = value\n        self.verbose = verbose\n\n    def on_epoch_end(self, epoch, logs={}):\n        current = logs.get(self.monitor)\n        if current is None:\n            warnings.warn(\"Early stopping requires %s available!\" % self.monitor, RuntimeWarning)\n\n        if current >= self.value:\n            if self.verbose > 0:\n                print(\"Epoch %05d: early stopping\" % epoch)\n            self.model.stop_training = True\n\n#Save large model using pickle formate instead of h5            \nclass SaveCheckPoint(Callback):\n    def __init__(self, model, dest_folder):\n        super(Callback, self).__init__()\n        self.model = model\n        self.dest_folder = dest_folder\n        \n        #initiate\n        self.best_val_acc = 0\n        self.best_val_loss = sys.maxsize #get max value\n          \n    def on_epoch_end(self, epoch, logs={}):\n        val_acc = logs['val_acc']\n        val_loss = logs['val_loss']\n\n        if val_acc > self.best_val_acc:\n            self.best_val_acc = val_acc\n            \n            # Save weights in pickle format instead of h5\n            print('\\nSaving val_acc %f at %s' %(self.best_val_acc, self.dest_folder))\n            weigh= self.model.get_weights()\n\n            #now, use pickle to save your model weights, instead of .h5\n            #for heavy model architectures, .h5 file is unsupported.\n            fpkl= open(self.dest_folder, 'wb') #Python 3\n            pickle.dump(weigh, fpkl, protocol= pickle.HIGHEST_PROTOCOL)\n            fpkl.close()\n            \n#             model.save('tmp.h5')\n        elif val_acc == self.best_val_acc:\n            if val_loss < self.best_val_loss:\n                self.best_val_loss=val_loss\n                \n                # Save weights in pickle format instead of h5\n                print('\\nSaving val_acc %f at %s' %(self.best_val_acc, self.dest_folder))\n                weigh= self.model.get_weights()\n\n                #now, use pickle to save your model weights, instead of .h5\n                #for heavy model architectures, .h5 file is unsupported.\n                fpkl= open(self.dest_folder, 'wb') #Python 3\n                pickle.dump(weigh, fpkl, protocol= pickle.HIGHEST_PROTOCOL)\n                fpkl.close()","metadata":{"id":"7lTNZK9tZ79l","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ls -l","metadata":{"id":"qWmgrvKLZ79m","outputId":"dfdc321d-4295-4344-bc42-95d6f598d579","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Non-Groups\n#Split training and validation\n#Using Expert Data\n\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping, CSVLogger, ReduceLROnPlateau\nfrom tensorflow.keras.optimizers import Adamax\n# from keras.utils import multi_gpu_model\n\nimport time, os\nfrom math import ceil\nimport multiprocessing\n\ntrain_datagen = ImageDataGenerator(\n    rescale = 1./255,\n    width_shift_range=0.15,\n    height_shift_range=0.2,\n    zoom_range=0.35,\n    channel_shift_range=15,##\n    fill_mode='nearest'\n    #preprocessing_function=preprocess_input,\n)\n\ntest_datagen = ImageDataGenerator(\n    rescale = 1./255\n    #preprocessing_function = preprocess_input\n)\n\nNUM_GPU = 1\nbatch_size = 8\ntrain_set = train_datagen.flow_from_directory('../input/railway-track-fault-detection-dataset1-rail/Dataset 1_Rail/Train',\n                                                 target_size = (600, 600),\n                                                 batch_size = batch_size,\n                                                 class_mode = 'binary',\n                                                 shuffle=True,\n                                                 seed=10,\n#                                                  subset=\"training\"\n                                              )\n\nvalid_set = test_datagen.flow_from_directory('../input/railway-track-fault-detection-dataset1-rail/Dataset 1_Rail/Validiation',\n                                                 target_size = (600,600),\n                                                 batch_size = batch_size,\n                                                 class_mode = 'binary',\n                                                 shuffle=True,\n                                                 seed=10,\n#                                                  subset=\"validation\"\n                                             )\n\nmodel_txt = 'st'\n# Helper: Save the model.\nsavedfilename = os.path.join('checkpoints', 'Railway_IRESNET_LSTM.hdf5')\n\ncheckpointer = ModelCheckpoint(savedfilename,\n                          monitor='val_acc', verbose=1, \n                          save_best_only=True, mode='max',save_weights_only=True)########\n\n# Helper: TensorBoard\ntb = TensorBoard(log_dir=os.path.join('svhn_output', 'logs', model_txt))\n\n# Helper: Save results.\ntimestamp = time.time()\ncsv_logger = CSVLogger(os.path.join('svhn_output', 'logs', model_txt + '-' + 'training-' + \\\n    str(timestamp) + '.log'))\n\nearlystopping = EarlyStoppingByAccVal(monitor='val_acc', value=0.9900, verbose=1)\n\n#Using multiple models if more than 1 GPU\nif NUM_GPU != 1:\n    model_mul = multi_gpu_model(model_1, gpus=NUM_GPU)\nelse:\n    model_mul = model_1\n    \nepochs = 50 ##!!!\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5,\n                              patience=5, min_lr=0.001)\n#decay = reduce_lr/epochs\noptimizer = Adamax()\n\nmodel_mul.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['accuracy'])\n\nstep_size_train=ceil(train_set.n/train_set.batch_size)\nstep_size_valid=ceil(valid_set.n/valid_set.batch_size)\n# step_size_test=ceil(testing_set.n//testing_set.batch_size)\ntf.keras.backend.set_learning_phase(1)\n\nresult = model_mul.fit_generator(\n    generator = train_set, \n    steps_per_epoch = step_size_train,\n    validation_data = valid_set,\n    validation_steps = step_size_valid,\n    shuffle=True,\n    epochs=epochs,\n    callbacks=[reduce_lr],\n    #callbacks=[tb, csv_logger, checkpointer, earlystopping],        \n    verbose=1)\ntf.keras.backend.set_learning_phase(0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nprint(result.history['val_accuracy'])\nprint(np.mean(result.history['val_accuracy']))\nprint(np.max(result.history['val_accuracy']))\nprint(result.history['accuracy'])\nprint(np.mean(result.history['accuracy']))\nprint(np.max(result.history['accuracy']))","metadata":{"id":"CNyMQqZoZ79n","outputId":"1e1f1225-35ee-4761-964f-a637001164ce","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nhistory = result\n# summarize history for accuracy\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.grid()\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","metadata":{"id":"DQjZ8RUzZ79n","outputId":"a83a4880-72a7-4bd6-a184-9e6aad149de3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\nimport numpy as np\nimg = cv2.imread('../input/railwaytrackv4/Dataset _ Railway Track Fault Detection-20210713T183411Z-001/Dataset _ Railway Track Fault Detection/Test/Defective/IMG_20201114_101200.jpg')\nplt.imshow(img)\nimg = cv2.resize(img,(400,400))\nimg = np.reshape(img,[400,400,3])\n\npred_value = model_mul.predict_generator(img)\n\nprint(pred_value)\nif pred_value>0.5:\n    print(\"This Railway track has no fault\")\nelse:\n    print(\"This Railway track has fault\")","metadata":{"id":"46Z09TZ1Z79n","outputId":"48c2aebc-6279-4be1-ed5c-2720438e02db","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\nimport numpy as np\nimg = cv2.imread('../input/railwaytrackv4/Dataset _ Railway Track Fault Detection-20210713T183411Z-001/Dataset _ Railway Track Fault Detection/Test/Defective/aug_prefix_0_2872.jpg')\nplt.imshow(img)\nimg = cv2.resize(img,(650,650))\nimg = np.reshape(img,[1,650,650,3])\n\nclasses = model_mul.predict(img)\n\nprint(classes)\nif classes>0.5:\n    print(\"This Railway track has no fault\")\nelse:\n    print(\"This Railway track has fault\")","metadata":{"id":"WAFR-VeqZ79n","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\nimport numpy as np\nimg = cv2.imread('../input/railwaytrackv4/Dataset _ Railway Track Fault Detection-20210713T183411Z-001/Dataset _ Railway Track Fault Detection/Test/Defective/aug_prefix_0_2958.jpg')\nplt.imshow(img)\nimg = cv2.resize(img,(650,650))\nimg = np.reshape(img,[1,650,650,3])\n\npred_value = model_mul.predict_generator(img)\n\nprint(pred_value)\nif pred_value>0.5:\n    print(\"This Railway track has no fault\")\nelse:\n    print(\"This Railway track has fault\")","metadata":{"id":"GrmGwo1CEuZ8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\nimport numpy as np\nimg = cv2.imread('../input/railwaytrackv4/Dataset _ Railway Track Fault Detection-20210713T183411Z-001/Dataset _ Railway Track Fault Detection/Test/Non Defective/IMG_20201114_102909.jpg')\nplt.imshow(img)\nimg = cv2.resize(img,(650,650))\nimg = np.reshape(img,[1,650,650,3])\n\nclasses = model_mul.predict(img)\n\nprint(classes)\nif classes>0.5:\n    print(\"This Railway track has no fault\")\nelse:\n    print(\"This Railway track has fault\")","metadata":{"id":"wy6NX8kHZ79o","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\nimport numpy as np\nimg = cv2.imread('../input/railwaytrackv4/Dataset _ Railway Track Fault Detection-20210713T183411Z-001/Dataset _ Railway Track Fault Detection/Test/Non Defective/aug_prefix_0_3373.jpg')\nplt.imshow(img)\nimg = cv2.resize(img,(650,650))\nimg = np.reshape(img,[1,650,650,3])\n\npred_value = model_mul.predict_generator(img)\n\nprint(pred_value)\nif pred_value>0.5:\n    print(\"This Railway track has no fault\")\nelse:\n    print(\"This Railway track has fault\")","metadata":{"id":"eUChd05KEuZ8","trusted":true},"execution_count":null,"outputs":[]}]}
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --upgrade pip\n!pip install tensorflow","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","id":"I-65SOeQZ79d","outputId":"7c7e5e8d-ea7c-4fda-bd3c-b574b4b20444","execution":{"iopub.status.busy":"2021-12-11T16:05:25.009554Z","iopub.execute_input":"2021-12-11T16:05:25.010324Z","iopub.status.idle":"2021-12-11T16:05:58.167881Z","shell.execute_reply.started":"2021-12-11T16:05:25.010228Z","shell.execute_reply":"2021-12-11T16:05:58.166974Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report,confusion_matrix\nimport cv2\nimport tensorflow as tf\nimport seaborn as sns\nimport os","metadata":{"execution":{"iopub.status.busy":"2021-12-11T16:05:58.170616Z","iopub.execute_input":"2021-12-11T16:05:58.171194Z","iopub.status.idle":"2021-12-11T16:06:03.411218Z","shell.execute_reply.started":"2021-12-11T16:05:58.171151Z","shell.execute_reply":"2021-12-11T16:06:03.410392Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Defining each of these directories","metadata":{"id":"GLfwYtthZ79g"}},{"cell_type":"code","source":"import numpy as np\nlabels = ['Defective', 'Non Defective']\nimg_size = 224\ndef get_data(data_dir):\n    data = [] \n    for label in labels: \n        path = os.path.join(data_dir, label)\n        class_num = labels.index(label)\n        for img in os.listdir(path):\n            try:\n                img_arr = cv2.imread(os.path.join(path, img))[...,::-1] #convert BGR to RGB format\n                resized_arr = cv2.resize(img_arr, (img_size, img_size)) # Reshaping images to preferred size\n                data.append([resized_arr, class_num])\n            except Exception as e:\n                print(e)\n    return np.array(data)","metadata":{"execution":{"iopub.status.busy":"2021-12-11T16:06:03.412740Z","iopub.execute_input":"2021-12-11T16:06:03.412994Z","iopub.status.idle":"2021-12-11T16:06:03.421735Z","shell.execute_reply.started":"2021-12-11T16:06:03.412959Z","shell.execute_reply":"2021-12-11T16:06:03.420824Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"train = get_data('../input/railwaytrackv4/Dataset _ Railway Track Fault Detection-20210713T183411Z-001/Dataset _ Railway Track Fault Detection/Train')\nval = get_data('../input/railwaytrackv4/Dataset _ Railway Track Fault Detection-20210713T183411Z-001/Dataset _ Railway Track Fault Detection/Validation')","metadata":{"execution":{"iopub.status.busy":"2021-12-11T16:06:03.424016Z","iopub.execute_input":"2021-12-11T16:06:03.424710Z","iopub.status.idle":"2021-12-11T16:10:19.958400Z","shell.execute_reply.started":"2021-12-11T16:06:03.424668Z","shell.execute_reply":"2021-12-11T16:10:19.957492Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"l = []\nfor i in train:\n    if(i[1] == 0):\n        l.append(\"Defective\")\n    else:\n        l.append(\"Non Defective\")\nsns.set_style('whitegrid')\nsns.countplot(l)","metadata":{"execution":{"iopub.status.busy":"2021-12-11T16:10:19.959898Z","iopub.execute_input":"2021-12-11T16:10:19.960194Z","iopub.status.idle":"2021-12-11T16:10:20.206566Z","shell.execute_reply.started":"2021-12-11T16:10:19.960147Z","shell.execute_reply":"2021-12-11T16:10:20.205815Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"from matplotlib import pyplot as plt\nplt.figure(figsize = (5,5))\nplt.imshow(train[1][0])\nplt.title(labels[train[0][1]])","metadata":{"execution":{"iopub.status.busy":"2021-12-11T16:10:20.207895Z","iopub.execute_input":"2021-12-11T16:10:20.208359Z","iopub.status.idle":"2021-12-11T16:10:20.686847Z","shell.execute_reply.started":"2021-12-11T16:10:20.208319Z","shell.execute_reply":"2021-12-11T16:10:20.686147Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (5,5))\nplt.imshow(train[-1][0])\nplt.title(labels[train[-1][1]])","metadata":{"execution":{"iopub.status.busy":"2021-12-11T16:10:20.688367Z","iopub.execute_input":"2021-12-11T16:10:20.688866Z","iopub.status.idle":"2021-12-11T16:10:21.148310Z","shell.execute_reply.started":"2021-12-11T16:10:20.688826Z","shell.execute_reply":"2021-12-11T16:10:21.147554Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"x_train = []\ny_train = []\nx_val = []\ny_val = []\n\nfor feature, label in train:\n  x_train.append(feature)\n  y_train.append(label)\n\nfor feature, label in val:\n  x_val.append(feature)\n  y_val.append(label)\n\n# Normalize the data\nx_train = np.array(x_train) / 255\nx_val = np.array(x_val) / 255\n\nx_train.reshape(-1, img_size, img_size, 1)\ny_train = np.array(y_train)\n\nx_val.reshape(-1, img_size, img_size, 1)\ny_val = np.array(y_val)","metadata":{"execution":{"iopub.status.busy":"2021-12-11T16:10:21.150453Z","iopub.execute_input":"2021-12-11T16:10:21.151501Z","iopub.status.idle":"2021-12-11T16:10:21.970821Z","shell.execute_reply.started":"2021-12-11T16:10:21.151461Z","shell.execute_reply":"2021-12-11T16:10:21.970082Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping, CSVLogger, ReduceLROnPlateau\nfrom tensorflow.keras.optimizers import Adamax\n\ndatagen = ImageDataGenerator(\n    rescale = 1./255,\n    width_shift_range=0.15,\n    height_shift_range=0.2,\n    zoom_range=0.35,\n    channel_shift_range=15,##\n    fill_mode='nearest')\ndatagen.fit(x_train)","metadata":{"execution":{"iopub.status.busy":"2021-12-11T16:10:21.972269Z","iopub.execute_input":"2021-12-11T16:10:21.972514Z","iopub.status.idle":"2021-12-11T16:10:23.605520Z","shell.execute_reply.started":"2021-12-11T16:10:21.972481Z","shell.execute_reply":"2021-12-11T16:10:23.604583Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"#  Building the Model","metadata":{"id":"qt8ShRrkZ79j"}},{"cell_type":"code","source":"#MUL 1 - Inception - ST\n#import keras\n# from keras.applications import InceptionV3\n# from keras.applications import Xception\nfrom tensorflow.keras.applications import InceptionResNetV2\n#from keras.applications import V2\n#model = VGG16()\nfrom  tensorflow.keras.models import Model\n# from keras.layers import concatenate\nfrom  tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Embedding, LSTM, GRU, Reshape, Concatenate,Bidirectional, Conv2D, MaxPooling2D, UpSampling2D, Flatten\n#from  tensorflow.keras.applications.mobilenet import preprocess_input\n\nfrom  tensorflow.keras.layers import GaussianNoise\ninput_img = Input(shape = (img_size,img_size, 3))\n#f1_x = tensorflow.keras.layers.Flatten()\n#encoder\nf1_x = Conv2D(128, (6, 6), activation='relu', kernel_initializer='he_normal', padding='valid')(input_img) #28 x 28 x 32\ntf.keras.layers.BatchNormalization()\ntf.keras.layers.Dropout(rate=0.4)\nf1_x = MaxPooling2D(pool_size=(2, 2))(f1_x) #14 x 14 x 32\ntf.keras.layers.BatchNormalization()\ntf.keras.layers.Dropout(rate=0.35)\nf1_x = Conv2D(64, (4, 4), activation='relu', kernel_initializer='he_normal', padding='valid')(f1_x) #14 x 14 x 64\ntf.keras.layers.BatchNormalization()\ntf.keras.layers.Dropout(rate=0.33)\nf1_x = MaxPooling2D(pool_size=(2, 2))(f1_x) #7 x 7 x 64\ntf.keras.layers.BatchNormalization()\ntf.keras.layers.Dropout(rate=0.3)\nf1_x = Conv2D(64, (4, 4), activation='relu', padding='valid',kernel_initializer='he_normal')(f1_x) #7 x 7 x 128 (small and thick)\ntf.keras.layers.BatchNormalization()\ntf.keras.layers.Dropout(rate=0.4)\nf1_x = MaxPooling2D(pool_size=(2, 2))(f1_x)\ntf.keras.layers.BatchNormalization()\ntf.keras.layers.Dropout(rate=0.49)\n#decoder\nf1_x = Conv2D(64, (6, 6), activation='relu', padding='valid',kernel_initializer='he_normal')(f1_x) #7 x 7 x 128\ntf.keras.layers.BatchNormalization()\ntf.keras.layers.Dropout(rate=0.36)\nf1_x = UpSampling2D((2,2))(f1_x) # 14 x 14 x 128\ntf.keras.layers.BatchNormalization()\ntf.keras.layers.Dropout(rate=0.38)\nf1_x = Conv2D(128, (6, 6), activation='relu', padding='valid',kernel_initializer='he_normal')(f1_x) # 14 x 14 x 64\ntf.keras.layers.BatchNormalization()\n#dropout=0.2\nf1_x = UpSampling2D((2,2))(f1_x) # 28 x 28 x 64\ntf.keras.layers.BatchNormalization()\ntf.keras.layers.Dropout(rate=0.37)\nf1_x = Conv2D(1, (6, 6), activation='sigmoid', padding='valid', kernel_initializer='he_normal')(f1_x) # 28 x 28 x 1\ntf.keras.layers.BatchNormalization()\ntf.keras.layers.BatchNormalization()\n\ntf.keras.layers.Dropout(rate=0.5)\n#f1_x = MinPooling2D(pool_size=(3, 3))(f1_x)     \n# f1_base = Xception(weights='imagenet', include_top=False, input_shape=(450,450,3))\n# f1_base = InceptionV3(weights='imagenet', include_top=False, input_shape=(299,299,3))\nf1_base = InceptionResNetV2(weights='imagenet', include_top=False, input_shape=(224,224,3))\ntf.keras.layers.Dropout(rate=0.45)\ntf.keras.layers.BatchNormalization()\ntf.keras.layers.Dropout(rate=0.5)\ntf.keras.layers.BatchNormalization()\nf1_x = f1_base.output\ntf.keras.layers.BatchNormalization()\nf1_x = GlobalAveragePooling2D()(f1_x)\ntf.keras.layers.BatchNormalization()\ntf.keras.layers.Dropout(rate=0.4)\n#f1_x = Conv2D(128, (3, 3), activation='relu', padding='same')(f1_x) #7 x 7 x 128\n#f1_x = MaxPooling2D(pool_size=(2, 2))(f1_x) \nf1_x = Reshape([1,1536])(f1_x)  \nf1_x = Bidirectional(tf.keras.layers.LSTM(2900, \nreturn_sequences=False, \ndropout=0.55),\ninput_shape=[1,1024],\nmerge_mode='concat')(f1_x)\ntf.keras.layers.BatchNormalization()\n#f1_x = Conv2D(128, (3, 3), activation='relu', padding='same')(f1_x) #7 x 7 x 128\n#f1_x = MaxPooling2D(pool_size=(2, 2))(f1_x) \n#Regularization with noise\nf1_x = GaussianNoise(0.20)(f1_x)\ntf.keras.layers.BatchNormalization()\nf1_x = Dense(100,activation='relu', kernel_initializer='he_normal',kernel_regularizer=tf.keras.regularizers.l1_l2(0.01),kernel_constraint=tf.keras.constraints.max_norm(1.))(f1_x)\ntf.keras.layers.BatchNormalization()\ntf.keras.layers.Dropout(rate=0.5)\ntf.keras.layers.BatchNormalization()\nf1_x = Dense(100,activation='relu', kernel_initializer='he_normal',kernel_constraint=tf.keras.constraints.max_norm(1.))(f1_x)\ntf.keras.layers.Dropout(rate=0.58)\ntf.keras.layers.BatchNormalization()\ntf.keras.layers.BatchNormalization()\ntf.keras.layers.BatchNormalization()\ntf.keras.layers.Dropout(rate=0.46)\ntf.keras.layers.BatchNormalization()\n#he_avg_int = keras.initializers.VarianceScaling(scale=2, mode = 'fan_avg', distribution='uniform')\n#f1_x = Conv2D(64, (3, 3), activation='relu', padding='same')(f1_x)\nf1_x = Dense(1, activation='sigmoid', kernel_initializer='he_normal')(f1_x)\nmodel = Model(inputs=[f1_base.input],outputs=[f1_x])\nprint(model.summary())","metadata":{"id":"Un0AmXeWEuZ4","outputId":"9f92fadb-9b04-4d0c-d5b9-ba6a410a7a7a","execution":{"iopub.status.busy":"2021-12-11T16:10:23.608622Z","iopub.execute_input":"2021-12-11T16:10:23.608882Z","iopub.status.idle":"2021-12-11T16:11:17.103429Z","shell.execute_reply.started":"2021-12-11T16:10:23.608846Z","shell.execute_reply":"2021-12-11T16:11:17.102731Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"## fix for multi_gpu_model prediction time longer\nfrom tensorflow.keras.layers import Lambda, concatenate\nfrom tensorflow.keras import Model\n\n\ndef multi_gpu_model(model, gpus):\n    if isinstance(gpus, (list, tuple)):\n        num_gpus = len(gpus)\n        target_gpu_ids = gpus\n    else:\n        num_gpus = gpus\n        target_gpu_ids = range(num_gpus)\n\n    def get_slice(data, i, parts):\n        shape = tf.shape(data)\n        batch_size = shape[:1]\n        input_shape = shape[1:]\n        step = batch_size // parts\n        if i == num_gpus - 1:\n            size = batch_size - step * i\n        else:\n            size = step\n        size = tf.concat([size, input_shape], axis=0)\n        stride = tf.concat([step, input_shape * 0], axis=0)\n        start = stride * i\n        return tf.slice(data, start, size)\n\n    all_outputs = []\n    for i in range(len(model.outputs)):\n        all_outputs.append([])\n\n    # Place a copy of the model on each GPU,\n    # each getting a slice of the inputs.\n    for i, gpu_id in enumerate(target_gpu_ids):\n        with tf.device('/gpu:%d' % gpu_id):\n            with tf.name_scope('replica_%d' % gpu_id):\n                inputs = []\n                # Retrieve a slice of the input.\n                for x in model.inputs:\n                    input_shape = tuple(x.get_shape().as_list())[1:]\n                    slice_i = Lambda(get_slice,\n                                   output_shape=input_shape,\n                                   arguments={'i': i,\n                                              'parts': num_gpus})(x)\n                    inputs.append(slice_i)\n\n                # Apply model on slice\n                # (creating a model replica on the target device).\n                outputs = model(inputs)\n                if not isinstance(outputs, list):\n                    outputs = [outputs]\n\n                # Save the outputs for merging back together later.\n                for o in range(len(outputs)):\n                    all_outputs[o].append(outputs[o])\n\n    # Merge outputs on CPU.\n    with tf.device('/cpu:0'):\n        merged = []\n        for name, outputs in zip(model.output_names, all_outputs):\n            merged.append(concatenate(outputs,\n                                    axis=0, name=name))\n        return Model(model.inputs, merged)","metadata":{"execution":{"iopub.status.busy":"2021-12-11T16:11:17.105626Z","iopub.execute_input":"2021-12-11T16:11:17.105826Z","iopub.status.idle":"2021-12-11T16:11:17.119080Z","shell.execute_reply.started":"2021-12-11T16:11:17.105801Z","shell.execute_reply":"2021-12-11T16:11:17.118347Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import Callback\nimport pickle\nimport sys\n\n#Stop training on val_acc\nclass EarlyStoppingByAccVal(Callback):\n    def __init__(self, monitor='val_acc', value=0.00001, verbose=1):\n        super(Callback, self).__init__()\n        self.monitor = monitor\n        self.value = value\n        self.verbose = verbose\n\n    def on_epoch_end(self, epoch, logs={}):\n        current = logs.get(self.monitor)\n        if current is None:\n            warnings.warn(\"Early stopping requires %s available!\" % self.monitor, RuntimeWarning)\n\n        if current >= self.value:\n            if self.verbose > 0:\n                print(\"Epoch %05d: early stopping\" % epoch)\n            self.model.stop_training = True\n\n#Save large model using pickle formate instead of h5            \nclass SaveCheckPoint(Callback):\n    def __init__(self, model, dest_folder):\n        super(Callback, self).__init__()\n        self.model = model\n        self.dest_folder = dest_folder\n        \n        #initiate\n        self.best_val_acc = 0\n        self.best_val_loss = sys.maxsize #get max value\n          \n    def on_epoch_end(self, epoch, logs={}):\n        val_acc = logs['val_acc']\n        val_loss = logs['val_loss']\n\n        if val_acc > self.best_val_acc:\n            self.best_val_acc = val_acc\n            \n            # Save weights in pickle format instead of h5\n            print('\\nSaving val_acc %f at %s' %(self.best_val_acc, self.dest_folder))\n            weigh= self.model.get_weights()\n\n            #now, use pickle to save your model weights, instead of .h5\n            #for heavy model architectures, .h5 file is unsupported.\n            fpkl= open(self.dest_folder, 'wb') #Python 3\n            pickle.dump(weigh, fpkl, protocol= pickle.HIGHEST_PROTOCOL)\n            fpkl.close()\n            \n#             model.save('tmp.h5')\n        elif val_acc == self.best_val_acc:\n            if val_loss < self.best_val_loss:\n                self.best_val_loss=val_loss\n                \n                # Save weights in pickle format instead of h5\n                print('\\nSaving val_acc %f at %s' %(self.best_val_acc, self.dest_folder))\n                weigh= self.model.get_weights()\n\n                #now, use pickle to save your model weights, instead of .h5\n                #for heavy model architectures, .h5 file is unsupported.\n                fpkl= open(self.dest_folder, 'wb') #Python 3\n                pickle.dump(weigh, fpkl, protocol= pickle.HIGHEST_PROTOCOL)\n                fpkl.close()","metadata":{"execution":{"iopub.status.busy":"2021-12-11T16:11:17.120500Z","iopub.execute_input":"2021-12-11T16:11:17.120755Z","iopub.status.idle":"2021-12-11T16:11:17.139603Z","shell.execute_reply.started":"2021-12-11T16:11:17.120720Z","shell.execute_reply":"2021-12-11T16:11:17.138657Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger, ReduceLROnPlateau\nfrom tensorflow.keras.optimizers import Adamax\n# from keras.utils import multi_gpu_model\n\nearlystopping = EarlyStoppingByAccVal(monitor='val_acc', value=0.9900, verbose=1)\n\n    \n ##!!!\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5,\n                              patience=5, min_lr=0.001)\n#decay = reduce_lr/epochs\noptimizer = Adamax()\n\nmodel.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['accuracy'])\n\n\nresult = model.fit(\n    x_train,y_train,validation_data = (x_val, y_val),\n    epochs = 100,\n    callbacks=[reduce_lr],\n    #callbacks=[tb, csv_logger, checkpointer, earlystopping],        \n    verbose=1)","metadata":{"execution":{"iopub.status.busy":"2021-12-11T16:11:17.142023Z","iopub.execute_input":"2021-12-11T16:11:17.142315Z","iopub.status.idle":"2021-12-11T16:41:48.622828Z","shell.execute_reply.started":"2021-12-11T16:11:17.142288Z","shell.execute_reply":"2021-12-11T16:41:48.621964Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nprint(result.history['val_accuracy'])\nprint(np.mean(result.history['val_accuracy']))\nprint(np.max(result.history['val_accuracy']))\nprint(result.history['accuracy'])\nprint(np.mean(result.history['accuracy']))\nprint(np.max(result.history['accuracy']))","metadata":{"id":"CNyMQqZoZ79n","outputId":"1e1f1225-35ee-4761-964f-a637001164ce","execution":{"iopub.status.busy":"2021-12-11T16:41:48.624960Z","iopub.execute_input":"2021-12-11T16:41:48.625200Z","iopub.status.idle":"2021-12-11T16:41:48.633053Z","shell.execute_reply.started":"2021-12-11T16:41:48.625173Z","shell.execute_reply":"2021-12-11T16:41:48.632298Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nhistory = result\n# summarize history for accuracy\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('training and validation accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.grid()\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Training and Validation loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","metadata":{"id":"DQjZ8RUzZ79n","outputId":"a83a4880-72a7-4bd6-a184-9e6aad149de3","execution":{"iopub.status.busy":"2021-12-11T16:41:48.636293Z","iopub.execute_input":"2021-12-11T16:41:48.636832Z","iopub.status.idle":"2021-12-11T16:41:49.171597Z","shell.execute_reply.started":"2021-12-11T16:41:48.636791Z","shell.execute_reply":"2021-12-11T16:41:49.170925Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict_x = model.predict(x_val)\nclasses_x = np.argmax(predict_x, axis=1)\npredictions = classes_x.reshape(1,-1)[0]\nprint(classification_report(y_val, predictions, target_names = ['Defective (Class 0)','Non Defective (Class 1)']))","metadata":{"execution":{"iopub.status.busy":"2021-12-11T16:41:49.172989Z","iopub.execute_input":"2021-12-11T16:41:49.173511Z","iopub.status.idle":"2021-12-11T16:41:55.123488Z","shell.execute_reply.started":"2021-12-11T16:41:49.173471Z","shell.execute_reply":"2021-12-11T16:41:55.122741Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"print(confusion_matrix(y_val, predictions))","metadata":{"execution":{"iopub.status.busy":"2021-12-11T16:41:55.125037Z","iopub.execute_input":"2021-12-11T16:41:55.125579Z","iopub.status.idle":"2021-12-11T16:41:55.136464Z","shell.execute_reply.started":"2021-12-11T16:41:55.125538Z","shell.execute_reply":"2021-12-11T16:41:55.135286Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"import cv2\nimport numpy as np\nimg = cv2.imread('../input/railway-track-fault-detection-dataset1-rail/Dataset 1_Rail/Test/Defective/IMG_20201114_101134.jpg')\nplt.imshow(img)\nimg = cv2.resize(img,(224,224))\nimg = np.reshape(img,[1,224,224,3])\n\nclasses = model.predict(img)\n\nprint(classes)\nif classes>0.5:\n    print(\"This Railway track has no fault\")\nelse:\n    print(\"This Railway track has fault\")","metadata":{"id":"46Z09TZ1Z79n","outputId":"48c2aebc-6279-4be1-ed5c-2720438e02db","execution":{"iopub.status.busy":"2021-12-11T16:41:55.138363Z","iopub.execute_input":"2021-12-11T16:41:55.138645Z","iopub.status.idle":"2021-12-11T16:41:55.937784Z","shell.execute_reply.started":"2021-12-11T16:41:55.138606Z","shell.execute_reply":"2021-12-11T16:41:55.933761Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"import cv2\nimport numpy as np\nimg = cv2.imread('../input/railway-track-fault-detection-dataset1-rail/Dataset 1_Rail/Test/Defective/es5n3o3y5cu11.jpg')\nplt.imshow(img)\nimg = cv2.resize(img,(224,224))\nimg = np.reshape(img,[1,224,224,3])\n\nclasses = model.predict(img)\n\nprint(classes)\nif classes>0.5:\n    print(\"This Railway track has no fault\")\nelse:\n    print(\"This Railway track has fault\")","metadata":{"id":"WAFR-VeqZ79n","execution":{"iopub.status.busy":"2021-12-11T16:41:55.939066Z","iopub.status.idle":"2021-12-11T16:41:55.939526Z","shell.execute_reply.started":"2021-12-11T16:41:55.939282Z","shell.execute_reply":"2021-12-11T16:41:55.939309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\nimport numpy as np\nimg = cv2.imread('../input/railway-track-fault-detection-dataset1-rail/Dataset 1_Rail/Validiation/Non Defective/IMG_20201114_100758.jpg')\nplt.imshow(img)\nimg = cv2.resize(img,(224,224))\nimg = np.reshape(img,[1,224,224,3])\n\nclasses = model.predict(img)\n\nprint(classes)\nif classes>0.005:\n    print(\"This Railway track has no fault\")\nelse:\n    print(\"This Railway track has fault\")","metadata":{"id":"GrmGwo1CEuZ8","execution":{"iopub.status.busy":"2021-12-11T16:41:55.941885Z","iopub.status.idle":"2021-12-11T16:41:55.942322Z","shell.execute_reply.started":"2021-12-11T16:41:55.942078Z","shell.execute_reply":"2021-12-11T16:41:55.942102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\nimport numpy as np\nimg = cv2.imread('../input/railway-track-fault-detection-dataset1-rail/Dataset 1_Rail/Test/Non Defective/IMG_20201114_102051.jpg')\nplt.imshow(img)\nimg = cv2.resize(img,(224,224))\nimg = np.reshape(img,[1,224,224,3])\n\nclasses = model.predict(img)\n\nprint(classes)\nif classes>0.005:\n    print(\"This Railway track has no fault\")\nelse:\n    print(\"This Railway track has fault\")","metadata":{"id":"wy6NX8kHZ79o","execution":{"iopub.status.busy":"2021-12-11T16:41:55.945073Z","iopub.status.idle":"2021-12-11T16:41:55.945460Z","shell.execute_reply.started":"2021-12-11T16:41:55.945257Z","shell.execute_reply":"2021-12-11T16:41:55.945279Z"},"trusted":true},"execution_count":null,"outputs":[]}]}